name: "develop - Deploy infra"

on:
  pull_request:
    branches:
      - develop
    types: [opened, synchronize]
    paths-ignore:
      - 'README.md'
  workflow_dispatch:
    branches:
      - develop

permissions:
  contents: read
  pull-requests: write
  #issues: write

env:
  # Possible values: https://developer.hashicorp.com/terraform/internals/debugging
  TF_LOG: INFO
  # Credentials for deployment to AWS
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  # S3 bucket for the Terraform state
  S3_BUCKET_TF_STATE: "develop-test-llm-terraform-state"
  TF_STATE_FILE: "vpc-eks.tfstate"
  AWS_REGION: "us-east-1"
  TERRAFORM_VERSION: "1.7.0"
  # https://docs.nginx.com/nginx-ingress-controller/technical-specifications/#supported-kubernetes-versions
  NGINX_INGRESS_CONTROLLER_CHART_VERSION: "1.0.2"
  # https://artifacthub.io/packages/helm/metrics-server/metrics-server
  METRICS_SERVER_CHART_VERSION: "3.12.0"
  # https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack
  KUBE_PROMETHEUS_STACK_CHART_VERSION: "56.21.2"
  ENVIRONMENT: develop
  EKS_CLUSTER_NAME: develop-eks-cluster
  TEST_NAMESPACE: "demo-test"
  # https://docs.crossplane.io/latest/software/install/
  CROSSPLANE_CHART_VERSION: "1.15.1"
  # https://marketplace.upbound.io/providers/upbound/provider-aws-s3/v1.2.1
  UPBOUND_PROVIDER_AWS_S3_VERSION: "v1.2.1"

jobs:
  # Deploy the VPC and the EKS cluster
  deploy_vpc_and_eks:
    name: "Deploy VPC and EKS cluster"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    defaults:
      run:
        working-directory: terraform-code/vpc-eks
    outputs:
      tfplanExitCode: ${{ steps.tf-plan.outputs.exitcode }}
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: $TERRAFORM_VERSION
      
      - name: Install checkov
        run: pip install checkov    

      - name: Terraform Format
        run: terraform fmt

      - name: Terraform Init
        run: |
          terraform init \
            -backend-config "bucket=$S3_BUCKET_TF_STATE" \
            -backend-config "key=$TF_STATE_FILE"

      - name: Terraform Validate
        run: terraform validate -no-color

      - name: Fetch variable file infra.tfvars from CONFIG REPO
        uses: actions/checkout@v2
        with:
          repository: "luisllm/environments"
          ref: develop # or specify the branch, tag, or commit hash where the file resides
          path: "./environments"
      
      - name: Print variable file infra.tfvars coming from CONFIG REPO
        run: cat ../../environments/tf-config/infra.tfvars
         
      # Generates an execution plan for Terraform
      # An exit code of 0 indicated no changes, 1 a terraform failure, 2 there are pending changes.
      - name: Terraform Plan
        id: tf-plan
        run: |
          export exitcode=0
          terraform plan -var-file="../../environments/tf-config/infra.tfvars" -detailed-exitcode -no-color -out tfplan || export exitcode=$?

          echo "exitcode=$exitcode" >> $GITHUB_OUTPUT
          if [ $exitcode -eq 1 ]; then
            echo Terraform Plan Failed!
            exit 1
          else 
            exit 0
          fi

      - name: Run checkov
        run: checkov -d . --quiet --soft-fail

      # Terraform Apply
      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan

      # Notify to Slack channel if it fails
      #- name: Notify slack fail
      #    if: failure()
      #    env:
      #      SLACK_BOT_TOKEN: ${{ secrets.SLACK_NOTIFICATIONS_BOT_TOKEN }}
      #    uses: voxmedia/github-action-slack-notify-build@v1
      #    with:
      #      channel: app-alerts
      #      status: FAILED
      #      color: danger


  # Deploy Prometheus and Grafana
  deploy_monitoring:
    name: "Deploy monitoring"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    needs: [deploy_vpc_and_eks]
    defaults:
      run:
        # In here I have a custom values file for prometheus and grafana, where I customized some grafana dashboards
        working-directory: monitoring
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Create monitoring namespace
        run: |
          kubectl get namespace | grep -q "^monitoring" || kubectl create namespace monitoring  

      # Using a custom values to introduce some custom Grafana dashboards
      - name: Deploy Prometheus and Grafana with Helm
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          helm upgrade --install "mon" \
            prometheus-community/kube-prometheus-stack \
            --version $KUBE_PROMETHEUS_STACK_CHART_VERSION \
            --namespace monitoring \
            --values custom-prometheus-grafana-values.yaml


  # Deploy metrics-server to be able to have HPA working
  # https://artifacthub.io/packages/helm/metrics-server/metrics-server
  metrics_server_deployment:
    name: "Deploy metrics-server"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    needs: [deploy_vpc_and_eks]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Deploy metrics-server
        run: |
          helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
          helm upgrade --install metrics-server metrics-server/metrics-server \
            --version $METRICS_SERVER_CHART_VERSION \
            --namespace kube-system
            


  # Deploy nginx ingress controller to be able to expose the EKS cluster via a public AWS LB, and be able to send requests
  ingress_deployment:
    name: "Deploy nginx ingress controller"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    needs: [deploy_vpc_and_eks]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      # Get from AWS ParameterStore the SecGroup ID that should be attached to the public AWS LB
      # The SecGroup was created by Terraform
      - name: Get SecGroup from ParameterStore
        id: discover-lb-secgroup
        run: |
          secgroup_id=$(aws ssm get-parameter --name "/$ENVIRONMENT/public-lb-secgroup-id" --query 'Parameter.Value' --output text)
          echo "SECGROUP_ID=$secgroup_id" >>$GITHUB_ENV

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Create public-ingress namespace
        run: |
          kubectl get namespace | grep -q "^public-ingress" || kubectl create namespace public-ingress  

      - name: Deploy nginx ingress controller
        run: |
          helm upgrade --install public-ingress \
            oci://ghcr.io/nginxinc/charts/nginx-ingress \
            --version $NGINX_INGRESS_CONTROLLER_CHART_VERSION \
            --namespace public-ingress \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-security-groups"="$SECGROUP_ID"

  

  # Store in AWS ParameterStore the public LB dns name created automatically when the nginx ingress controller was deployed. It will be used to send requests
  public_ingress_lb_discovery:
    name: "Store ingress AWS public LB dns name in AWS ParameterStore"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    needs: [ingress_deployment]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      # Wait for 1min for the LB to get created
      - name: Wait for 1 Minute
        run: sleep 60

      - name: Discover ingress public LB DNS Name and store it in AWS ParameterStore
        id: discover-lb-dns
        run: |
          # Get the list of load balancer names
          load_balancer_names=$(aws elb describe-load-balancers --query "LoadBalancerDescriptions[].LoadBalancerName" --output text)
          
          # Iterate over each load balancer name
          for lb_name in $load_balancer_names; do
              # Retrieve tags for the current load balancer
              tags=$(aws elb describe-tags --load-balancer-names "$lb_name" --query "TagDescriptions[].Tags[?Key=='kubernetes.io/cluster/$EKS_CLUSTER_NAME'].Value" --output text)
              
              # Check if the load balancer has the desired tag
              if [ ! -z "$tags" ]; then
                  # If the tag is found, print the DNS name of the load balancer
                  ingress_lb_dns_name=$(aws elb describe-load-balancers --load-balancer-names "$lb_name" --query "LoadBalancerDescriptions[].DNSName" --output text)
                  echo "$ingress_lb_dns_name"
                  aws ssm put-parameter --name "/$ENVIRONMENT/ingress-public-load-balancer-dns" --value "$ingress_lb_dns_name" --type String --overwrite
              fi
          done


  # Deploy Crossplane
  crossplane_deployment:
    name: "Deploy Crossplane"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    defaults:
      run:
        working-directory: crossplane
    needs: [deploy_vpc_and_eks]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Deploy Crossplane with Helm
        run: |
          helm repo add crossplane-stable https://charts.crossplane.io/stable
          helm repo update
          helm upgrade --install crossplane \
            --namespace crossplane-system \
            --create-namespace crossplane-stable/crossplane \
            --version $CROSSPLANE_CHART_VERSION

      - name: list files
        run: ls -ltr

      # Get from AWS ParameterStore the IAM Role ARN for the Crossplane ServiceAccount
      # The IAM Role was created by Terraform
      - name: Get IAM Role from ParameterStore
        id: discover-iam-role
        run: |
          iam_role_arn=$(aws ssm get-parameter --name "/$ENVIRONMENT/test-crossplane-iam-role-arn" --query 'Parameter.Value' --output text)
          echo "IAM_ROLE_ARN=$iam_role_arn" >>$GITHUB_ENV

      - name: Create Crossplane ControllerConfig, Provider and ProviderConfig 
        run: |
          sed -i "s/IAM_ROLE_ARN/'${IAM_ROLE_ARN}'/g" ControllerConfig.yaml > ControllerConfig-replaced.yaml
          kubectl apply -f ControllerConfig-replaced.yaml
          sed -i "s/UPBOUND_PROVIDER_AWS_S3_VERSION/'${UPBOUND_PROVIDER_AWS_S3_VERSION}'/g" Provider.yaml > Provider-replaced.yaml
          kubectl apply -f Provider-replaced.yaml
          kubectl apply -f ProviderConfig.yaml



  # Test and verify the infra was deployed correctly
  test_infra:
    name: "Test infra"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    needs: [deploy_vpc_and_eks, deploy_monitoring, ingress_deployment, public_ingress_lb_discovery, metrics_server_deployment, crossplane_deployment]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Check all nginx ingress controller PODs are running
        id: check_nginx_pods
        run: |
          kubectl get pods -n $TEST_NAMESPACE -l app.kubernetes.io/name=nginx-ingress -o jsonpath='{range .items[*]}{.status.phase}{"\n"}{end}' | grep -qv "Running" && echo "One or more pods are not in running state" && exit 1 || echo "All pods are running"



